{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca7b133",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-20T10:20:57.179243Z",
     "iopub.status.busy": "2024-01-20T10:20:57.178568Z",
     "iopub.status.idle": "2024-01-20T10:20:57.891967Z",
     "shell.execute_reply": "2024-01-20T10:20:57.890878Z"
    },
    "papermill": {
     "duration": 0.730601,
     "end_time": "2024-01-20T10:20:57.894357",
     "exception": false,
     "start_time": "2024-01-20T10:20:57.163756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24b437",
   "metadata": {
    "papermill": {
     "duration": 0.012757,
     "end_time": "2024-01-20T10:20:57.920244",
     "exception": false,
     "start_time": "2024-01-20T10:20:57.907487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**In this notebook and tutorial, we will fine-tune [Microsoft's Phi-2](https://huggingface.co/microsoft/phi-2) relatively small 2.7B model - which has \"showcased a nearly state-of-the-art performance among models with less than 13 billion parameters\" - *on your own data!***\n",
    "\n",
    "**Here we will use [QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314), a highly efficient fine-tuning technique that involves quantizing a pretrained LLM to just 4 bits and adding small “Low-Rank Adapters”. This unique approach allows for fine-tuning LLMs using just a single GPU! This technique is supported by the [PEFT library](https://huggingface.co/docs/peft/index).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60caf10",
   "metadata": {
    "papermill": {
     "duration": 0.012483,
     "end_time": "2024-01-20T10:20:57.945466",
     "exception": false,
     "start_time": "2024-01-20T10:20:57.932983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c9834",
   "metadata": {
    "papermill": {
     "duration": 0.013339,
     "end_time": "2024-01-20T10:20:57.971411",
     "exception": false,
     "start_time": "2024-01-20T10:20:57.958072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- [1- Install all the required libraries](#1)\n",
    "- [ 2 - Loading dataset](#2)\n",
    "- [ 3 - Create bitsandbytes configuration](#3)\n",
    "- [ 4 - Load Base Model](#4)\n",
    "- [ 5 - Tokenization](#5)\n",
    "- [ 6 - Test the Model with Zero Shot Inferencing](#6)\n",
    "- [ 7 - Pre-processing dataset](#7)\n",
    "- [ 8 - Setup the PEFT/LoRA model for Fine-Tuning](#8)\n",
    "- [ 9 - Train PEFT Adapter](#9)\n",
    "- [ 10 - Evaluate the Model Qualitatively (Human Evaluation)](#10)\n",
    "- [ 11 - Evaluate the Model Quantitatively (with ROUGE Metric)](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211517bf",
   "metadata": {
    "papermill": {
     "duration": 0.012582,
     "end_time": "2024-01-20T10:20:57.996570",
     "exception": false,
     "start_time": "2024-01-20T10:20:57.983988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a name='1'></a>\n",
    "#### 1. Install all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ea9312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:20:58.023571Z",
     "iopub.status.busy": "2024-01-20T10:20:58.023093Z",
     "iopub.status.idle": "2024-01-20T10:21:19.146843Z",
     "shell.execute_reply": "2024-01-20T10:21:19.145895Z"
    },
    "papermill": {
     "duration": 21.139912,
     "end_time": "2024-01-20T10:21:19.149213",
     "exception": false,
     "start_time": "2024-01-20T10:20:58.009301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc1d573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:21:19.177105Z",
     "iopub.status.busy": "2024-01-20T10:21:19.176788Z",
     "iopub.status.idle": "2024-01-20T10:21:19.181257Z",
     "shell.execute_reply": "2024-01-20T10:21:19.180432Z"
    },
    "papermill": {
     "duration": 0.020442,
     "end_time": "2024-01-20T10:21:19.183228",
     "exception": false,
     "start_time": "2024-01-20T10:21:19.162786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9653ecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:21:19.210099Z",
     "iopub.status.busy": "2024-01-20T10:21:19.209829Z",
     "iopub.status.idle": "2024-01-20T10:21:37.915536Z",
     "shell.execute_reply": "2024-01-20T10:21:37.914277Z"
    },
    "papermill": {
     "duration": 18.720742,
     "end_time": "2024-01-20T10:21:37.916969",
     "exception": true,
     "start_time": "2024-01-20T10:21:19.196227",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "getpass was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interpreter_login\n\u001b[0;32m---> 20\u001b[0m \u001b[43minterpreter_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/_login.py:189\u001b[0m, in \u001b[0;36minterpreter_login\u001b[0;34m(new_session, write_permission)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken can be pasted using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRight-Click\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 189\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mToken: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m add_to_git_credential \u001b[38;5;241m=\u001b[39m _ask_for_confirmation_no_tui(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd token as git credential?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    192\u001b[0m _login(token\u001b[38;5;241m=\u001b[39mtoken, add_to_git_credential\u001b[38;5;241m=\u001b[39madd_to_git_credential, write_permission\u001b[38;5;241m=\u001b[39mwrite_permission)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1176\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1175\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetpass was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: getpass was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b30b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:06.372914Z",
     "iopub.status.busy": "2024-01-20T06:37:06.372043Z",
     "iopub.status.idle": "2024-01-20T06:37:06.386043Z",
     "shell.execute_reply": "2024-01-20T06:37:06.385137Z",
     "shell.execute_reply.started": "2024-01-20T06:37:06.372877Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f2257",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='2'></a>\n",
    "#### 2. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d5eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:09.803154Z",
     "iopub.status.busy": "2024-01-20T06:37:09.802328Z",
     "iopub.status.idle": "2024-01-20T06:37:14.800161Z",
     "shell.execute_reply": "2024-01-20T06:37:14.799265Z",
     "shell.execute_reply.started": "2024-01-20T06:37:09.803122Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/neil-code/dialogsum-test\n",
    "huggingface_dataset_name = \"neil-code/dialogsum-test\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041aa8e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This is what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e343d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:22.699920Z",
     "iopub.status.busy": "2024-01-20T06:37:22.699549Z",
     "iopub.status.idle": "2024-01-20T06:37:22.706866Z",
     "shell.execute_reply": "2024-01-20T06:37:22.705981Z",
     "shell.execute_reply.started": "2024-01-20T06:37:22.699890Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d39889e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='3'></a>\n",
    "#### 3. Create bitsandbytes configuration\n",
    "\n",
    "**To load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll achieve this with BitesAndBytesConfig from the Transformers library. This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3b445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:33.444907Z",
     "iopub.status.busy": "2024-01-20T06:37:33.444536Z",
     "iopub.status.idle": "2024-01-20T06:37:33.451948Z",
     "shell.execute_reply": "2024-01-20T06:37:33.450994Z",
     "shell.execute_reply.started": "2024-01-20T06:37:33.444875Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748e1f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='4'></a>\n",
    "#### 4. Load Base Model\n",
    "Let's now load Phi-2 using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80deea28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:36.663390Z",
     "iopub.status.busy": "2024-01-20T06:37:36.662683Z",
     "iopub.status.idle": "2024-01-20T06:37:55.595724Z",
     "shell.execute_reply": "2024-01-20T06:37:55.594708Z",
     "shell.execute_reply.started": "2024-01-20T06:37:36.663357Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name='microsoft/phi-2'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e80ae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='5'></a>\n",
    "#### 5. Tokenization\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc130317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:07.191825Z",
     "iopub.status.busy": "2024-01-20T06:38:07.190976Z",
     "iopub.status.idle": "2024-01-20T06:38:09.259675Z",
     "shell.execute_reply": "2024-01-20T06:38:09.258772Z",
     "shell.execute_reply.started": "2024-01-20T06:38:07.191780Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208640a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:11.288479Z",
     "iopub.status.busy": "2024-01-20T06:38:11.288093Z",
     "iopub.status.idle": "2024-01-20T06:38:16.216599Z",
     "shell.execute_reply": "2024-01-20T06:38:16.215607Z",
     "shell.execute_reply.started": "2024-01-20T06:38:11.288438Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34fc4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:17.767613Z",
     "iopub.status.busy": "2024-01-20T06:38:17.766951Z",
     "iopub.status.idle": "2024-01-20T06:38:17.977517Z",
     "shell.execute_reply": "2024-01-20T06:38:17.976678Z",
     "shell.execute_reply.started": "2024-01-20T06:38:17.767578Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef89a9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='6'></a>\n",
    "#### 6. Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769cd77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:23.249577Z",
     "iopub.status.busy": "2024-01-20T06:38:23.248327Z",
     "iopub.status.idle": "2024-01-20T06:38:29.685369Z",
     "shell.execute_reply": "2024-01-20T06:38:29.684466Z",
     "shell.execute_reply.started": "2024-01-20T06:38:23.249528Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6d92a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='7'></a>\n",
    "#### 7. Pre-processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a142f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:41.361729Z",
     "iopub.status.busy": "2024-01-20T06:38:41.361011Z",
     "iopub.status.idle": "2024-01-20T06:38:41.368868Z",
     "shell.execute_reply": "2024-01-20T06:38:41.367673Z",
     "shell.execute_reply.started": "2024-01-20T06:38:41.361689Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86c7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:42.367542Z",
     "iopub.status.busy": "2024-01-20T06:38:42.366797Z",
     "iopub.status.idle": "2024-01-20T06:38:42.373767Z",
     "shell.execute_reply": "2024-01-20T06:38:42.372862Z",
     "shell.execute_reply.started": "2024-01-20T06:38:42.367511Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6378120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:44.012070Z",
     "iopub.status.busy": "2024-01-20T06:38:44.011319Z",
     "iopub.status.idle": "2024-01-20T06:38:44.018676Z",
     "shell.execute_reply": "2024-01-20T06:38:44.017721Z",
     "shell.execute_reply.started": "2024-01-20T06:38:44.012035Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfd6f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:45.504915Z",
     "iopub.status.busy": "2024-01-20T06:38:45.504201Z",
     "iopub.status.idle": "2024-01-20T06:38:45.509895Z",
     "shell.execute_reply": "2024-01-20T06:38:45.508897Z",
     "shell.execute_reply.started": "2024-01-20T06:38:45.504881Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38b25c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:49.770369Z",
     "iopub.status.busy": "2024-01-20T06:38:49.770020Z",
     "iopub.status.idle": "2024-01-20T06:39:02.630942Z",
     "shell.execute_reply": "2024-01-20T06:39:02.630214Z",
     "shell.execute_reply.started": "2024-01-20T06:38:49.770342Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851c068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:02.632587Z",
     "iopub.status.busy": "2024-01-20T06:39:02.632296Z",
     "iopub.status.idle": "2024-01-20T06:39:02.637741Z",
     "shell.execute_reply": "2024-01-20T06:39:02.636760Z",
     "shell.execute_reply.started": "2024-01-20T06:39:02.632561Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {train_dataset.shape}\")\n",
    "print(f\"Validation: {eval_dataset.shape}\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cf76b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='8'></a>\n",
    "#### 8. Setup the PEFT/LoRA model for Fine-Tuning\n",
    "Now, let's perform Parameter Efficient Fine-Tuning (PEFT) fine-tuning. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning.\n",
    "PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, in essence, enables efficient model fine-tuning using fewer computational resources, often achievable with just a single GPU. Following LoRA fine-tuning for a specific task or use case, the outcome is an unchanged original LLM and the emergence of a considerably smaller \"LoRA adapter,\" often representing a single-digit percentage of the original LLM size (in MBs rather than GBs).\n",
    "\n",
    "During inference, the LoRA adapter must be combined with its original LLM. The advantage lies in the ability of many LoRA adapters to reuse the original LLM, thereby reducing overall memory requirements when handling multiple tasks and use cases.\n",
    "\n",
    "Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained.\n",
    "r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "alpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82efdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:20.294539Z",
     "iopub.status.busy": "2024-01-20T06:39:20.293676Z",
     "iopub.status.idle": "2024-01-20T06:39:20.303534Z",
     "shell.execute_reply": "2024-01-20T06:39:20.302434Z",
     "shell.execute_reply.started": "2024-01-20T06:39:20.294505Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003c0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:24.093412Z",
     "iopub.status.busy": "2024-01-20T06:39:24.092536Z",
     "iopub.status.idle": "2024-01-20T06:39:24.100571Z",
     "shell.execute_reply": "2024-01-20T06:39:24.099579Z",
     "shell.execute_reply.started": "2024-01-20T06:39:24.093380Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c5789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:31.284063Z",
     "iopub.status.busy": "2024-01-20T06:39:31.283152Z",
     "iopub.status.idle": "2024-01-20T06:39:31.698890Z",
     "shell.execute_reply": "2024-01-20T06:39:31.697918Z",
     "shell.execute_reply.started": "2024-01-20T06:39:31.284029Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574645b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bcb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:35.271906Z",
     "iopub.status.busy": "2024-01-20T06:39:35.271014Z",
     "iopub.status.idle": "2024-01-20T06:39:35.283574Z",
     "shell.execute_reply": "2024-01-20T06:39:35.282621Z",
     "shell.execute_reply.started": "2024-01-20T06:39:35.271872Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc60a57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:41.791778Z",
     "iopub.status.busy": "2024-01-20T06:39:41.790943Z",
     "iopub.status.idle": "2024-01-20T06:39:41.807977Z",
     "shell.execute_reply": "2024-01-20T06:39:41.807104Z",
     "shell.execute_reply.started": "2024-01-20T06:39:41.791743Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See how the model looks different now, with the LoRA adapters added:\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952dc7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='9'></a>\n",
    "#### 9. Train PEFT Adapter\n",
    "\n",
    "Define training arguments and create Trainer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f33b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:49.182295Z",
     "iopub.status.busy": "2024-01-20T06:39:49.181607Z",
     "iopub.status.idle": "2024-01-20T06:39:49.194550Z",
     "shell.execute_reply": "2024-01-20T06:39:49.193629Z",
     "shell.execute_reply.started": "2024-01-20T06:39:49.182259Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = './peft-dialogue-summary-training/final-checkpoint'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947ce49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:56.204084Z",
     "iopub.status.busy": "2024-01-20T06:39:56.203399Z",
     "iopub.status.idle": "2024-01-20T06:39:56.209885Z",
     "shell.execute_reply": "2024-01-20T06:39:56.208967Z",
     "shell.execute_reply.started": "2024-01-20T06:39:56.204052Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2e79b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:57.949207Z",
     "iopub.status.busy": "2024-01-20T06:39:57.948300Z",
     "iopub.status.idle": "2024-01-20T10:11:40.582353Z",
     "shell.execute_reply": "2024-01-20T10:11:40.581400Z",
     "shell.execute_reply.started": "2024-01-20T06:39:57.949172Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad525d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:09.229339Z",
     "iopub.status.busy": "2024-01-20T10:12:09.228538Z",
     "iopub.status.idle": "2024-01-20T10:12:09.234735Z",
     "shell.execute_reply": "2024-01-20T10:12:09.233805Z",
     "shell.execute_reply.started": "2024-01-20T10:12:09.229305Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e07b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:15.717375Z",
     "iopub.status.busy": "2024-01-20T10:12:15.716634Z",
     "iopub.status.idle": "2024-01-20T10:12:16.006683Z",
     "shell.execute_reply": "2024-01-20T10:12:16.005744Z",
     "shell.execute_reply.started": "2024-01-20T10:12:15.717341Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "del original_model\n",
    "del peft_trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701c631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:19.432308Z",
     "iopub.status.busy": "2024-01-20T10:12:19.431935Z",
     "iopub.status.idle": "2024-01-20T10:12:19.437916Z",
     "shell.execute_reply": "2024-01-20T10:12:19.436995Z",
     "shell.execute_reply.started": "2024-01-20T10:12:19.432277Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7e784",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='10'></a>\n",
    "#### 10. Evaluate the Model Qualitatively (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7c9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:23.438323Z",
     "iopub.status.busy": "2024-01-20T10:12:23.437949Z",
     "iopub.status.idle": "2024-01-20T10:12:27.860662Z",
     "shell.execute_reply": "2024-01-20T10:12:27.859789Z",
     "shell.execute_reply.started": "2024-01-20T10:12:23.438291Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a26de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:35.287195Z",
     "iopub.status.busy": "2024-01-20T10:12:35.286321Z",
     "iopub.status.idle": "2024-01-20T10:12:35.492183Z",
     "shell.execute_reply": "2024-01-20T10:12:35.491261Z",
     "shell.execute_reply.started": "2024-01-20T10:12:35.287161Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df66d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:39.215138Z",
     "iopub.status.busy": "2024-01-20T10:12:39.214472Z",
     "iopub.status.idle": "2024-01-20T10:12:39.745376Z",
     "shell.execute_reply": "2024-01-20T10:12:39.744360Z",
     "shell.execute_reply.started": "2024-01-20T10:12:39.215104Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ba4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:13:40.909063Z",
     "iopub.status.busy": "2024-01-20T10:13:40.908658Z",
     "iopub.status.idle": "2024-01-20T10:13:49.707535Z",
     "shell.execute_reply": "2024-01-20T10:13:49.706576Z",
     "shell.execute_reply.started": "2024-01-20T10:13:40.909027Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "\n",
    "peft_model_res = gen(ft_model,prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('#End')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343ef30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='11'></a>\n",
    "#### 11. Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32415e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:13:54.919616Z",
     "iopub.status.busy": "2024-01-20T10:13:54.918729Z",
     "iopub.status.idle": "2024-01-20T10:13:58.903914Z",
     "shell.execute_reply": "2024-01-20T10:13:58.903111Z",
     "shell.execute_reply.started": "2024-01-20T10:13:54.919578Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3063381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:16:20.440910Z",
     "iopub.status.busy": "2024-01-20T10:16:20.440537Z",
     "iopub.status.idle": "2024-01-20T10:18:49.394779Z",
     "shell.execute_reply": "2024-01-20T10:18:49.393788Z",
     "shell.execute_reply.started": "2024-01-20T10:16:20.440879Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    #print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
    "    \n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e829f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:18:49.396915Z",
     "iopub.status.busy": "2024-01-20T10:18:49.396626Z",
     "iopub.status.idle": "2024-01-20T10:19:04.322742Z",
     "shell.execute_reply": "2024-01-20T10:19:04.321453Z",
     "shell.execute_reply.started": "2024-01-20T10:18:49.396890Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544be228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:19:30.797808Z",
     "iopub.status.busy": "2024-01-20T10:19:30.796882Z",
     "iopub.status.idle": "2024-01-20T10:19:32.043480Z",
     "shell.execute_reply": "2024-01-20T10:19:32.042466Z",
     "shell.execute_reply.started": "2024-01-20T10:19:30.797774Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca9279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:19:35.851880Z",
     "iopub.status.busy": "2024-01-20T10:19:35.851035Z",
     "iopub.status.idle": "2024-01-20T10:19:35.857618Z",
     "shell.execute_reply": "2024-01-20T10:19:35.856633Z",
     "shell.execute_reply.started": "2024-01-20T10:19:35.851846Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 47.590297,
   "end_time": "2024-01-20T10:21:41.280535",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-20T10:20:53.690238",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
